{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"home\"></a>\n",
    "\n",
    "[1- Tokenizer](#1)\n",
    "\n",
    "[2- Preprocessing](#2)\n",
    "\n",
    "[3- Stemming](#3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from collections import Counter\n",
    "from math import log10\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## Tokenizers\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokenizer(txt):\n",
    "    return [token for token in txt.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(txt,regex='\\w+'):\n",
    "    reg = nltk.RegexpTokenizer(regex)\n",
    "    return reg.tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## Preprocessing\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_remove(tokens):\n",
    "    stop = nltk.corpus.stopwords.words('english')\n",
    "    return [term.lower() for term in tokens if term.lower() not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## Stemming\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stem(tokens):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    return[porter.stem(term) for term in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lancester_stem(tokens):\n",
    "    # lancester = nltk.PorterStemmer()\n",
    "    lancester = nltk.LancasterStemmer()\n",
    "    return[lancester.stem(term) for term in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## Files Reader\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(folder_path):\n",
    "\n",
    "    docs_content = dict()\n",
    "\n",
    "    # Ensure the path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all files in the directory\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        for file_name in files:\n",
    "            # Check if the file is a text file (you can modify the condition as needed)\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "                # Read the text from the file\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text_content = file.read()\n",
    "\n",
    "                # Store the text content in the dictionary\n",
    "                docs_content[file_name.replace(\".txt\",\"\")] = text_content\n",
    "\n",
    "    return docs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D1': 'Researchers have successfully applied large language models (LLMs) such as ChatGPT to reranking in an information retrieval context, but to date, such work has mostly been built on proprietary models hidden behind opaque API endpoints. This approach yields experimental results that are not reproducible and non-deterministic, threatening the veracity of outcomes that build on such shaky foundations. To address this significant shortcoming, we present RankVicuna, the first fully open-source LLM capable of performing high-quality listwise reranking in a zero-shot setting. Experimental results on the TREC 2019 and 2020 Deep Learning Tracks show that we can achieve effectiveness comparable to zero-shot reranking with GPT-3.5 with a much smaller 7B parameter model, although our effectiveness remains slightly behind reranking with GPT-4. We hope our work provides the foundation for future research on reranking with modern LLMs.',\n",
       " 'D2': 'With the advent of transformer-based architectures, the contextual representation of text data has leveraged the query and the document to be represented in low-dimensional dense vector space. These vectors are learned embeddings of fixed sizes, resulting in deeper text understanding. In this study, we designed a pipeline for effectively retrieving documents from a large search space by combining the deeper text understanding capabilities of the transformer-based BERT model and a phrase embedding-based query expansion model. To learn the contextual representations, we fine-tuned a deep semantic matching model by separately encoding the document and the query. The encoder model is based on the Sentence BERT (SBERT) architecture, which separately generates dense vector representations of documents and queries. The study has also addressed the maximum token length limitation of transformer-based models through the summarization of lengthy documents. In addition, to improve the clarity and completeness of short queries and reduce the semantic gap, a phrase embedding-based query expansion model is employed. The documents and their dense vectors are indexed using the Elasticsearch engine, and matched them with query vectors for retrieving query-specific documents. Finally, the BERT-based cross-encoder model is used to re-rank the relevant records for each query. It performs full self-attention over the inputs, and yields richer text interactions to produce the final results. To assess performance, experiments are conducted on two well-known datasets, TREC-CDS-2014 and OHSUMED. A comparative analysis is carried out, which clearly demonstrates that the proposed framework produced competitive retrieval results.',\n",
       " 'D3': 'In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high cost and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task, Scientific DOcument Retrieval using Multi-level Aspect-based quEries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for validating the performance of Large Language Models (LLMs) on expert-level dataset annotation tasks. LLM annotation of the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, the DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights the need for better approaches to handle complex, multifaceted queries in scientific research.',\n",
       " 'D4': 'Large Language Models (LLMs) demonstrate impressive effectiveness in zero-shot document ranking tasks. Pointwise, Pairwise, and Listwise prompting approaches have been proposed for LLM-based zero-shot ranking. Our study begins by thoroughly evaluating these existing approaches within a consistent experimental framework, considering factors like model size, token consumption, latency, among others. This first-of-its-kind comparative evaluation of these approaches allows us to identify the trade-offs between effectiveness and efficiency inherent in each approach. We find that while Pointwise approaches score high on efficiency, they suffer from poor effectiveness. Conversely, Pairwise approaches demonstrate superior effectiveness but incur high computational overhead. To further enhance the efficiency of LLM-based zero-shot ranking, we propose a novel Setwise prompting approach. Our approach reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, significantly improving the efficiency of LLM-based zero-shot ranking. We test our method using the TREC DL datasets and the BEIR zero-shot document ranking benchmark. The empirical results indicate that our approach considerably reduces computational costs while also retaining high zero-shot ranking effectiveness. ',\n",
       " 'D5': 'Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that captures the position of a query/document in the tree. We demonstrate the effectiveness of EHI on several benchmarks, including de-facto industry standard MS MARCO (Dev set and TREC DL19) datasets. For example, with the same compute budget, EHI outperforms state-of-the-art (SOTA) in by 0.6% (MRR@10) on MS MARCO dev set and by 4.2% (nDCG@10) on TREC DL19 benchmarks. ',\n",
       " 'D6': 'Query expansion (QE) is commonly used to improve the performance of traditional information retrieval (IR) models. With the adoption of deep learning in IR research, neural QE models have emerged in recent years. Many of these models focus on learning embeddings by leveraging query-document relevance. These embedding models allow computing semantic similarities between queries and documents to generate expansion terms. However, existing models often ignore query-document interactions. This research aims to address that gap by proposing a QE model using a conditional variational autoencoder. It first maps a query-document pair into a latent space based on their interaction, then estimates an expansion model from that latent space. The proposed model is trained on relevance feedback data and generates expansions using pseudo-relevance feedback at test time. The proposed model is evaluated on three standard TREC collections for document ranking: AP and Robust 04 and GOV02, and the MS MARCO dataset for passage ranking. Results show the model outperforms state-of-the-art traditional and neural QE models. It also demonstrates higher additivity with neural matching than baselines.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = read_docs(\"Collection\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## Files PreProcessor\n",
    "\n",
    "[Home](#home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_preprocessor(documents):\n",
    "    \n",
    "    \"\"\"This function applies tokenization ,stop words removal and stemming\n",
    "    it takes as input documents which is a dictionnary with values as documents content\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    #{file_name:dict of frequencies,file_name:dict of frequencies}\n",
    "    all_docs_frequencies_split_port=dict()\n",
    "    all_docs_frequencies_split_lancaster=dict()\n",
    "    all_docs_frequencies_regex_port=dict()\n",
    "    all_docs_frequencies_regex_lancaster=dict()\n",
    "    \n",
    "    for name , content in documents.items():\n",
    "        \n",
    "        #get tokens\n",
    "        splitted_tokens = split_tokenizer(content)\n",
    "        # regex_tokens = regex_tokenizer(content,\"\\w+(?:-\\w+)*(?:\\.\\w+)?(?:\\@\\w+)?\")\n",
    "        # regex_tokens = regex_tokenizer(content,\"\\w+(?:\\/\\w+)*(?:-\\w+)*(?:\\,\\w+)?(?:\\.\\w+)?(?:\\@\\w+)?\")\n",
    "       \n",
    "       \n",
    "        regex_tokens = regex_tokenizer(content,\"\\w+(?:\\/\\w+)*(?:-\\w+)*(?:\\,\\w+)?(?:\\.\\w+)?(?:\\@\\w+)?%?\")\n",
    "        # regex_tokens = regex_tokenizer(content,\"(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,]\\d+)?%?|\\w+(?:[\\-/]\\w+)*\")\n",
    "       \n",
    "       \n",
    "        # regex_tokens = regex_tokenizer(content,\"\\b(?:\\w+@\\w+|\\w+\\.\\d+|\\w+-\\w+\\.\\w+|\\w+-?\\w+-?\\w*-?\\w+|\\w+)\\b\")\n",
    "        \n",
    "        #remove stopwords\n",
    "        splitted_tokens = stop_remove(splitted_tokens)\n",
    "        regex_tokens = stop_remove(regex_tokens)\n",
    "        \n",
    "        #stemming\n",
    "        \n",
    "        splitted_tokens_porter = porter_stem(splitted_tokens)\n",
    "        splitted_tokens_lancester = lancester_stem(splitted_tokens)\n",
    "        \n",
    "        regex_tokens_porter = porter_stem(regex_tokens)\n",
    "        regex_tokens_lancester = lancester_stem(regex_tokens)\n",
    "        \n",
    "        #frequencies\n",
    "        \n",
    "        all_docs_frequencies_split_port[name] = dict(Counter(splitted_tokens_porter))\n",
    "        all_docs_frequencies_split_lancaster[name] = dict(Counter(splitted_tokens_lancester))\n",
    "        all_docs_frequencies_regex_port[name] = dict(Counter(regex_tokens_porter))\n",
    "        all_docs_frequencies_regex_lancaster[name] = dict(Counter(regex_tokens_lancester))\n",
    "        \n",
    "    \n",
    "    \n",
    "    descripteur = dict()\n",
    "    descripteur[\"split_port\"] = all_docs_frequencies_split_port\n",
    "    descripteur[\"split_lancester\"] = all_docs_frequencies_split_lancaster\n",
    "    descripteur[\"regex_port\"] = all_docs_frequencies_regex_port\n",
    "    descripteur[\"regex_lancester\"] = all_docs_frequencies_regex_lancaster\n",
    "    \n",
    "    return descripteur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'research': 2,\n",
       " 'success': 1,\n",
       " 'appli': 1,\n",
       " 'larg': 1,\n",
       " 'languag': 1,\n",
       " 'model': 2,\n",
       " '(llms)': 1,\n",
       " 'chatgpt': 1,\n",
       " 'rerank': 5,\n",
       " 'inform': 1,\n",
       " 'retriev': 1,\n",
       " 'context,': 1,\n",
       " 'date,': 1,\n",
       " 'work': 2,\n",
       " 'mostli': 1,\n",
       " 'built': 1,\n",
       " 'proprietari': 1,\n",
       " 'hidden': 1,\n",
       " 'behind': 2,\n",
       " 'opaqu': 1,\n",
       " 'api': 1,\n",
       " 'endpoints.': 1,\n",
       " 'approach': 1,\n",
       " 'yield': 1,\n",
       " 'experiment': 2,\n",
       " 'result': 2,\n",
       " 'reproduc': 1,\n",
       " 'non-deterministic,': 1,\n",
       " 'threaten': 1,\n",
       " 'verac': 1,\n",
       " 'outcom': 1,\n",
       " 'build': 1,\n",
       " 'shaki': 1,\n",
       " 'foundations.': 1,\n",
       " 'address': 1,\n",
       " 'signific': 1,\n",
       " 'shortcoming,': 1,\n",
       " 'present': 1,\n",
       " 'rankvicuna,': 1,\n",
       " 'first': 1,\n",
       " 'fulli': 1,\n",
       " 'open-sourc': 1,\n",
       " 'llm': 1,\n",
       " 'capabl': 1,\n",
       " 'perform': 1,\n",
       " 'high-qual': 1,\n",
       " 'listwis': 1,\n",
       " 'zero-shot': 2,\n",
       " 'setting.': 1,\n",
       " 'trec': 1,\n",
       " '2019': 1,\n",
       " '2020': 1,\n",
       " 'deep': 1,\n",
       " 'learn': 1,\n",
       " 'track': 1,\n",
       " 'show': 1,\n",
       " 'achiev': 1,\n",
       " 'effect': 2,\n",
       " 'compar': 1,\n",
       " 'gpt-3.5': 1,\n",
       " 'much': 1,\n",
       " 'smaller': 1,\n",
       " '7b': 1,\n",
       " 'paramet': 1,\n",
       " 'model,': 1,\n",
       " 'although': 1,\n",
       " 'remain': 1,\n",
       " 'slightli': 1,\n",
       " 'gpt-4.': 1,\n",
       " 'hope': 1,\n",
       " 'provid': 1,\n",
       " 'foundat': 1,\n",
       " 'futur': 1,\n",
       " 'modern': 1,\n",
       " 'llms.': 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_preprocessed = files_preprocessor(documents)\n",
    "# documents_preprocessed.keys()\n",
    "documents_preprocessed[\"split_port\"][\"D1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_occurences(descripteurs):\n",
    "    \n",
    "    all_terms_occ = dict()\n",
    "    \n",
    "    for descripteur , docs_freq_dict in descripteurs.items():\n",
    "        terms_occ = dict()\n",
    "        for doc_name , doc_freq in docs_freq_dict.items():\n",
    "            \n",
    "            for term , freq in doc_freq.items():\n",
    "                \n",
    "                terms_occ[term] = terms_occ.get(term,0) + 1 \n",
    "        \n",
    "        all_terms_occ[descripteur] = terms_occ  \n",
    "    \n",
    "    return all_terms_occ          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'research': 2,\n",
       " 'success': 1,\n",
       " 'appli': 1,\n",
       " 'larg': 4,\n",
       " 'languag': 3,\n",
       " 'model': 6,\n",
       " '(llms)': 3,\n",
       " 'chatgpt': 1,\n",
       " 'rerank': 1,\n",
       " 'inform': 2,\n",
       " 'retriev': 5,\n",
       " 'context,': 1,\n",
       " 'date,': 1,\n",
       " 'work': 1,\n",
       " 'mostli': 1,\n",
       " 'built': 1,\n",
       " 'proprietari': 1,\n",
       " 'hidden': 1,\n",
       " 'behind': 1,\n",
       " 'opaqu': 1,\n",
       " 'api': 1,\n",
       " 'endpoints.': 1,\n",
       " 'approach': 3,\n",
       " 'yield': 2,\n",
       " 'experiment': 2,\n",
       " 'result': 5,\n",
       " 'reproduc': 1,\n",
       " 'non-deterministic,': 1,\n",
       " 'threaten': 1,\n",
       " 'verac': 1,\n",
       " 'outcom': 1,\n",
       " 'build': 1,\n",
       " 'shaki': 1,\n",
       " 'foundations.': 1,\n",
       " 'address': 4,\n",
       " 'signific': 2,\n",
       " 'shortcoming,': 1,\n",
       " 'present': 1,\n",
       " 'rankvicuna,': 1,\n",
       " 'first': 2,\n",
       " 'fulli': 1,\n",
       " 'open-sourc': 1,\n",
       " 'llm': 3,\n",
       " 'capabl': 2,\n",
       " 'perform': 4,\n",
       " 'high-qual': 1,\n",
       " 'listwis': 2,\n",
       " 'zero-shot': 2,\n",
       " 'setting.': 1,\n",
       " 'trec': 4,\n",
       " '2019': 1,\n",
       " '2020': 1,\n",
       " 'deep': 3,\n",
       " 'learn': 4,\n",
       " 'track': 1,\n",
       " 'show': 2,\n",
       " 'achiev': 1,\n",
       " 'effect': 5,\n",
       " 'compar': 4,\n",
       " 'gpt-3.5': 1,\n",
       " 'much': 1,\n",
       " 'smaller': 1,\n",
       " '7b': 1,\n",
       " 'paramet': 1,\n",
       " 'model,': 1,\n",
       " 'although': 1,\n",
       " 'remain': 1,\n",
       " 'slightli': 1,\n",
       " 'gpt-4.': 1,\n",
       " 'hope': 1,\n",
       " 'provid': 1,\n",
       " 'foundat': 1,\n",
       " 'futur': 1,\n",
       " 'modern': 1,\n",
       " 'llms.': 1,\n",
       " 'advent': 1,\n",
       " 'transformer-bas': 1,\n",
       " 'architectures,': 1,\n",
       " 'contextu': 1,\n",
       " 'represent': 1,\n",
       " 'text': 1,\n",
       " 'data': 2,\n",
       " 'leverag': 2,\n",
       " 'queri': 4,\n",
       " 'document': 5,\n",
       " 'repres': 2,\n",
       " 'low-dimension': 1,\n",
       " 'dens': 2,\n",
       " 'vector': 1,\n",
       " 'space.': 2,\n",
       " 'embed': 3,\n",
       " 'fix': 1,\n",
       " 'sizes,': 1,\n",
       " 'deeper': 1,\n",
       " 'understanding.': 1,\n",
       " 'study,': 1,\n",
       " 'design': 2,\n",
       " 'pipelin': 1,\n",
       " 'search': 2,\n",
       " 'space': 2,\n",
       " 'combin': 1,\n",
       " 'understand': 1,\n",
       " 'bert': 1,\n",
       " 'phrase': 1,\n",
       " 'embedding-bas': 2,\n",
       " 'expans': 2,\n",
       " 'model.': 1,\n",
       " 'representations,': 1,\n",
       " 'fine-tun': 1,\n",
       " 'semant': 3,\n",
       " 'match': 2,\n",
       " 'separ': 1,\n",
       " 'encod': 2,\n",
       " 'query.': 2,\n",
       " 'base': 3,\n",
       " 'sentenc': 1,\n",
       " '(sbert)': 1,\n",
       " 'architecture,': 1,\n",
       " 'gener': 2,\n",
       " 'queries.': 2,\n",
       " 'studi': 2,\n",
       " 'also': 4,\n",
       " 'maximum': 1,\n",
       " 'token': 2,\n",
       " 'length': 1,\n",
       " 'limit': 1,\n",
       " 'summar': 1,\n",
       " 'lengthi': 1,\n",
       " 'documents.': 1,\n",
       " 'addition,': 1,\n",
       " 'improv': 3,\n",
       " 'clariti': 1,\n",
       " 'complet': 1,\n",
       " 'short': 1,\n",
       " 'reduc': 2,\n",
       " 'gap,': 1,\n",
       " 'employed.': 1,\n",
       " 'index': 2,\n",
       " 'use': 5,\n",
       " 'elasticsearch': 1,\n",
       " 'engine,': 1,\n",
       " 'query-specif': 1,\n",
       " 'finally,': 1,\n",
       " 'bert-bas': 1,\n",
       " 'cross-encod': 1,\n",
       " 're-rank': 1,\n",
       " 'relev': 4,\n",
       " 'record': 1,\n",
       " 'full': 1,\n",
       " 'self-attent': 1,\n",
       " 'inputs,': 1,\n",
       " 'richer': 1,\n",
       " 'interact': 1,\n",
       " 'produc': 2,\n",
       " 'final': 1,\n",
       " 'results.': 1,\n",
       " 'assess': 1,\n",
       " 'performance,': 1,\n",
       " 'experi': 1,\n",
       " 'conduct': 1,\n",
       " 'two': 2,\n",
       " 'well-known': 1,\n",
       " 'datasets,': 1,\n",
       " 'trec-cds-2014': 1,\n",
       " 'ohsumed.': 1,\n",
       " 'analysi': 1,\n",
       " 'carri': 1,\n",
       " 'out,': 1,\n",
       " 'clearli': 1,\n",
       " 'demonstr': 4,\n",
       " 'propos': 5,\n",
       " 'framework': 2,\n",
       " 'competit': 1,\n",
       " 'scientif': 1,\n",
       " 'research,': 2,\n",
       " 'abil': 1,\n",
       " 'complex,': 1,\n",
       " 'multifacet': 1,\n",
       " 'critical.': 1,\n",
       " 'exist': 3,\n",
       " 'evalu': 3,\n",
       " 'dataset': 3,\n",
       " 'task': 1,\n",
       " 'limited,': 1,\n",
       " 'primarili': 1,\n",
       " 'due': 1,\n",
       " 'high': 2,\n",
       " 'cost': 2,\n",
       " 'effort': 1,\n",
       " 'requir': 1,\n",
       " 'annot': 1,\n",
       " 'resourc': 1,\n",
       " 'complex': 1,\n",
       " 'this,': 1,\n",
       " 'novel': 2,\n",
       " 'task,': 1,\n",
       " 'multi-level': 1,\n",
       " 'aspect-bas': 1,\n",
       " '(doris-mae),': 1,\n",
       " 'handl': 1,\n",
       " 'natur': 1,\n",
       " 'user': 1,\n",
       " 'research.': 1,\n",
       " 'develop': 1,\n",
       " 'benchmark': 1,\n",
       " 'within': 2,\n",
       " 'field': 1,\n",
       " 'comput': 4,\n",
       " 'science,': 1,\n",
       " 'consist': 2,\n",
       " '100': 1,\n",
       " 'human-author': 1,\n",
       " 'cases.': 1,\n",
       " 'query,': 1,\n",
       " 'assembl': 1,\n",
       " 'collect': 2,\n",
       " 'score': 2,\n",
       " 'rank': 3,\n",
       " 'them.': 1,\n",
       " 'recogn': 1,\n",
       " 'labor': 1,\n",
       " 'expert': 1,\n",
       " 'annotation,': 1,\n",
       " 'introduc': 2,\n",
       " 'anno-gpt,': 1,\n",
       " 'scalabl': 1,\n",
       " 'valid': 1,\n",
       " 'expert-level': 1,\n",
       " 'tasks.': 2,\n",
       " 'doris-ma': 1,\n",
       " '500x': 1,\n",
       " 'reduct': 1,\n",
       " 'cost,': 1,\n",
       " 'without': 1,\n",
       " 'compromis': 1,\n",
       " 'quality.': 1,\n",
       " 'furthermore,': 1,\n",
       " 'multi-ti': 1,\n",
       " 'structur': 2,\n",
       " 'queries,': 1,\n",
       " 'extend': 1,\n",
       " '4,000': 1,\n",
       " 'sub-queri': 1,\n",
       " 'test': 3,\n",
       " 'case': 1,\n",
       " 'addit': 2,\n",
       " 'annotation.': 1,\n",
       " '17': 1,\n",
       " 'recent': 2,\n",
       " 'method': 3,\n",
       " 'doris-mae,': 1,\n",
       " 'observ': 1,\n",
       " 'notabl': 1,\n",
       " 'drop': 1,\n",
       " 'tradit': 2,\n",
       " 'datasets.': 2,\n",
       " 'highlight': 1,\n",
       " 'need': 1,\n",
       " 'better': 1,\n",
       " 'impress': 1,\n",
       " 'pointwise,': 1,\n",
       " 'pairwise,': 1,\n",
       " 'prompt': 1,\n",
       " 'llm-base': 1,\n",
       " 'ranking.': 2,\n",
       " 'begin': 1,\n",
       " 'thoroughli': 1,\n",
       " 'framework,': 1,\n",
       " 'consid': 1,\n",
       " 'factor': 1,\n",
       " 'like': 2,\n",
       " 'size,': 1,\n",
       " 'consumption,': 1,\n",
       " 'latency,': 1,\n",
       " 'among': 1,\n",
       " 'others.': 1,\n",
       " 'first-of-its-kind': 1,\n",
       " 'allow': 2,\n",
       " 'us': 1,\n",
       " 'identifi': 1,\n",
       " 'trade-off': 1,\n",
       " 'effici': 2,\n",
       " 'inher': 1,\n",
       " 'approach.': 1,\n",
       " 'find': 2,\n",
       " 'pointwis': 1,\n",
       " 'efficiency,': 1,\n",
       " 'suffer': 1,\n",
       " 'poor': 1,\n",
       " 'effectiveness.': 1,\n",
       " 'conversely,': 1,\n",
       " 'pairwis': 1,\n",
       " 'superior': 1,\n",
       " 'incur': 1,\n",
       " 'overhead.': 1,\n",
       " 'enhanc': 1,\n",
       " 'ranking,': 1,\n",
       " 'setwis': 1,\n",
       " 'number': 1,\n",
       " 'infer': 1,\n",
       " 'amount': 1,\n",
       " 'consumpt': 1,\n",
       " 'procedure,': 1,\n",
       " 'significantli': 1,\n",
       " 'dl': 1,\n",
       " 'beir': 1,\n",
       " 'benchmark.': 1,\n",
       " 'empir': 1,\n",
       " 'indic': 1,\n",
       " 'consider': 1,\n",
       " 'retain': 1,\n",
       " 'industri': 1,\n",
       " 'standard': 2,\n",
       " 'problems,': 1,\n",
       " 'obtain': 1,\n",
       " 'web': 1,\n",
       " 'given': 1,\n",
       " 'techniqu': 1,\n",
       " 'two-stag': 1,\n",
       " 'process:': 1,\n",
       " '(a)': 1,\n",
       " 'contrast': 1,\n",
       " 'train': 2,\n",
       " 'dual': 1,\n",
       " 'emb': 1,\n",
       " '(b)': 1,\n",
       " 'approxim': 1,\n",
       " 'nearest': 1,\n",
       " 'neighbor': 1,\n",
       " '(anns)': 1,\n",
       " 'similar': 2,\n",
       " 'stage': 1,\n",
       " 'disjoint;': 1,\n",
       " 'might': 1,\n",
       " 'ill-suit': 1,\n",
       " 'ann': 1,\n",
       " 'vice-versa,': 1,\n",
       " 'lead': 1,\n",
       " 'suboptim': 1,\n",
       " 'performance.': 1,\n",
       " 'work,': 1,\n",
       " 'end-to-end': 1,\n",
       " 'hierarch': 1,\n",
       " '--': 1,\n",
       " 'ehi': 1,\n",
       " 'jointli': 1,\n",
       " 'optim': 1,\n",
       " 'invert': 1,\n",
       " 'file': 1,\n",
       " '(ivf)': 1,\n",
       " 'style': 1,\n",
       " 'tree': 1,\n",
       " 'anns.': 1,\n",
       " 'ensur': 1,\n",
       " 'stabl': 1,\n",
       " 'discret': 1,\n",
       " 'tree-bas': 1,\n",
       " 'structure,': 1,\n",
       " 'notion': 1,\n",
       " 'path': 1,\n",
       " 'captur': 1,\n",
       " 'posit': 1,\n",
       " 'query/docu': 1,\n",
       " 'tree.': 1,\n",
       " 'sever': 1,\n",
       " 'benchmarks,': 1,\n",
       " 'includ': 1,\n",
       " 'de-facto': 1,\n",
       " 'ms': 2,\n",
       " 'marco': 2,\n",
       " '(dev': 1,\n",
       " 'set': 1,\n",
       " 'dl19)': 1,\n",
       " 'example,': 1,\n",
       " 'budget,': 1,\n",
       " 'outperform': 2,\n",
       " 'state-of-the-art': 2,\n",
       " '(sota)': 1,\n",
       " '0.6%': 1,\n",
       " '(mrr@10)': 1,\n",
       " 'dev': 1,\n",
       " '4.2%': 1,\n",
       " '(ndcg@10)': 1,\n",
       " 'dl19': 1,\n",
       " 'benchmarks.': 1,\n",
       " '(qe)': 1,\n",
       " 'commonli': 1,\n",
       " '(ir)': 1,\n",
       " 'models.': 1,\n",
       " 'adopt': 1,\n",
       " 'ir': 1,\n",
       " 'neural': 1,\n",
       " 'qe': 1,\n",
       " 'emerg': 1,\n",
       " 'years.': 1,\n",
       " 'mani': 1,\n",
       " 'focu': 1,\n",
       " 'query-docu': 1,\n",
       " 'relevance.': 1,\n",
       " 'terms.': 1,\n",
       " 'however,': 1,\n",
       " 'often': 1,\n",
       " 'ignor': 1,\n",
       " 'interactions.': 1,\n",
       " 'aim': 1,\n",
       " 'gap': 1,\n",
       " 'condit': 1,\n",
       " 'variat': 1,\n",
       " 'autoencoder.': 1,\n",
       " 'map': 1,\n",
       " 'pair': 1,\n",
       " 'latent': 1,\n",
       " 'interaction,': 1,\n",
       " 'estim': 1,\n",
       " 'feedback': 1,\n",
       " 'pseudo-relev': 1,\n",
       " 'time.': 1,\n",
       " 'three': 1,\n",
       " 'ranking:': 1,\n",
       " 'ap': 1,\n",
       " 'robust': 1,\n",
       " '04': 1,\n",
       " 'gov02,': 1,\n",
       " 'passag': 1,\n",
       " 'higher': 1,\n",
       " 'baselines.': 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_per_doc = unique_occurences(documents_preprocessed)\n",
    "term_per_doc[\"split_port\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(freq,max_freq,n,N=6):\n",
    "    \n",
    "    return (freq/max_freq)*log10((N/n)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descripteurs_writer(descripteurs,unique_freq,header='doc term frequency weight\\n'):\n",
    "    \n",
    "    \"\"\"\n",
    "    dicts -> name of dict : dict of frequencies of docs \n",
    "    \n",
    "    unique_freq -> number of documents in which each term appears\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for descripteur , docs_freq_dict in descripteurs.items():\n",
    "        \n",
    "        with open(f\"descripteurs/descripteur_{descripteur}.txt\",mode=\"w\") as file:\n",
    "            \n",
    "            file.write(header)\n",
    "            for doc_name , doc_freq in docs_freq_dict.items():\n",
    "                \n",
    "                lines = []\n",
    "                max_freq = max(doc_freq.values())\n",
    "                \n",
    "                for term , freq in doc_freq.items():\n",
    "               \n",
    "                     line = f\"{doc_name} {term} {freq} {weight(freq,max_freq,unique_freq[descripteur][term]):.4f}\\n\"    \n",
    "                     \n",
    "                     lines.append(line)\n",
    "                \n",
    "                file.writelines(lines)     \n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "descripteurs_writer(documents_preprocessed,term_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_descripteurs(folder_path):\n",
    "\n",
    "    docs_content = dict()\n",
    "\n",
    "    # Ensure the path is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all files in the directory\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        for file_name in files:\n",
    "            # Check if the file is a text file (you can modify the condition as needed)\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                content = pd.read_csv(file_path,sep=\" \")\n",
    "                # content = pd.read_csv(file_path,header=None,sep=\" \")\n",
    "                # Read the text from the file\n",
    "                # with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    \n",
    "                #     text_content = file.readlines()\n",
    "\n",
    "                # Store the text content in the dictionary\n",
    "                docs_content[file_name.replace(\".txt\",\"\")] = content\n",
    "\n",
    "    return docs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descripteurs = read_descripteurs(\"descripteurs\")\n",
    "descripteurs = read_descripteurs(\"inverse\")\n",
    "# descripteurs.keys()\n",
    "type(descripteurs[\"inverse_regex_lancester\"])\n",
    "# type(descripteurs[\"descripteur_regex_lancester\"])\n",
    "# descripteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(path):\n",
    "    \n",
    "    descripteurs = read_descripteurs(path)\n",
    "    \n",
    "    for desc , content in descripteurs.items():\n",
    "        content = content[[\"term\",\"doc\",\"frequency\",\"weight\"]]\n",
    "        \n",
    "        content.to_csv(f\"inverse/{desc.replace('descripteur','inverse')}.txt\",index = False,sep=\" \")\n",
    "        # content.to_csv(f\"inverse/{desc.replace('descripteur','inverse')}.txt\",index = False,header=False)\n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse(\"descripteurs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from back import load_descripteurs_and_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =load_descripteurs_and_inverse()\n",
    "inverse_doc = documents[\"inverse_regex_port\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_size(doc_name,inverse_doc):\n",
    "    return len(inverse_doc[inverse_doc[\"doc\"] == doc_name])\n",
    "\n",
    "def docs_mean(inverse_doc):\n",
    "    \n",
    "    docs = inverse_doc[\"doc\"].unique()\n",
    "    sum = 0\n",
    "    \n",
    "    for doc in docs:\n",
    "        sum += doc_size(doc,inverse_doc)\n",
    "    \n",
    "    return sum/len(docs)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mean(inverse_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_query(query):\n",
    "    \n",
    "    tokens = [f\"'{token}'\" if token not in (\"AND\", \"OR\", \"NOT\") else token.lower() for token in query.split()]\n",
    "    try:\n",
    "        eval(\" \".join(tokens))\n",
    "    except SyntaxError:\n",
    "        valid = False\n",
    "    else:valid = True\n",
    "    \n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_query(\"Terme AND NOT OR d\")\n",
    "# is_valid_query(\"Terme AND NOT d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile=[]\n",
    "for token in \"Terme AND NOT OR d\".split():\n",
    "    pile.insert(0,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'OR', 'NOT', 'AND', 'Terme']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(\"ff\")==\"str\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
